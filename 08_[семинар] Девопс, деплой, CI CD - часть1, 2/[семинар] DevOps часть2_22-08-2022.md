## Deploy часть 2.

Первоисточник: [Девопс, деплой, CI/CD. Часть 2 - YouTube](https://www.youtube.com/watch?v=5cbvqmDAOZA)

В части 1.  мы уже начали рассматривать наши два приложения на java и на python.  Код примеров в репозитории [GitHub - grigory51/shbr-devops](https://github.com/grigory51/shbr-devops). 

> **ПОЛЕЗНЫЕ КОМАНДЫ:**
> 
> `vagrant up` # запуск vagrnt
> 
> `vagrant ssh` # зайти в машину
> 
> `minikube start --driver=none` # запуск minikube
> 
> `kubectl get СУЩНОСТЬ` # перечень namespaces, pods, services и т.д. 
> 
> `kubectl create namespace ИМЯ` # создание namespaces
> 
> `kubectl -n shbr apply -f ИМЯ__deployment.yaml` # применение ИМЯ__deployment (+ Pods создаются), в файле ИМЯ__deployment.yaml задается масштабирование
> 
> `kubectl -n shbr apply -f ИМЯ_service.yaml` # применение ИМЯ_service (для открытия портов, записанных в файле ИМЯ_service.yaml, для обращения к контейнерам)
> 
> `kubectl -n ИМЯ_namespaces get pods` # перечень Pod в ИМЯ_namespaces
> 
> `kubectl -n shbr delete pod ИМЯ_pod` # удаление Pod
> 
> `watch -n 0.5 kubectl -n shbr get pod` # вывод в цикле
> 
> `vagrant halt` # остановить виртуальные машины

### 5. Утилизация ресурсов

 Как максимально употребить все ресурсы и деньги были потрачены не зря?

#### Bare-metal

Предположим у нас три одела, каждый из отделов имеет по три сервера за 1 млн.руб. Каждая школа задеплоила по одному приложению. Машины недоутилизированы.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-17-19-43-10-image.png" title="" alt="" data-align="center">

1. На одном железном сервере запущены контейнеры/приложения одного проект-отдела.

2. В случае использования пакетов, приложения в общем случа не изолированы друг от друга, конкурируют за ресурсы.

3. Минимальная утилизация - нет доступа к неиспользуемым ресурсам (один отдел не имеет доступа к железным машинам другого отдела).

**CAPEX (капитальные расходы)** - капитал, использующийся компаниями для приобретения или модернизации физических активов (сюда входят затраты на приобретение нового железа). Соответственно все стремятся эти расходы минимизировать, и не покупить новое железо.

Предлагается объединить все железные машины в единый пул серверов и дать возможность пользоваться ими всем отделам.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-17-20-14-19-image.png" title="" alt="" data-align="center">

Все машины намного больше утилизированы. Органичения кому сколько приложений запускать реализуется путем выделения квот, программно. Есть отдел, который предоставляет как сервис - платформу для развертывания приложений.

**Облако контейнеров:**

1. На одном железном сервере запущено множество раных контейнеров

2. Контейнеры изолированы друг от друга, имеют гарантии на ресурсы

3. Маскимальная утилизация

### 6. Системы оркестрации

Как сделать так, чтобы роботы работали за нас?

Чтобы раскладывать приложения по нашим серверам, т.к. это теперь является единым плоским списком, который доступен всем и мы теперь не знаем где теперь наше приложение запущено, и нам вцелом это не важно, чтобы это проворачивать существуют различные системы оркестрации.

**Человек** - самая простая система оркестрации, который может в ручном режиме сидеть и управлять тем, каке докер контейнеры и где запущены.

Посмотрим на примере ansible, как это могло бы выглядеть.

#### Ansible: файл inventory

В ansible есть супер важный файл - inventory,  в котором есть список хостов, которыми мы управляем. Соответсвено хостам можно назначать роли, т.е. каким-то машинам мы назначили роль  hadoop_nodes, другим app_nodes.

```
[hadoop_nodes]
srv-1
srv-2
srv-3

[app_nodes]
srv-4
srv-5
srv-6
```

Соответсвенно в ручном режиме  мы можем наблюдать, что трафик от пользователей увеличился и часть машин мы перекидываем в другую группу и раскатываем новую конфигурацию.

```
[hadoop_nodes]
srv-1

[app_nodes]
srv-2            # перенесли при росте нагрузки на приложение
srv-3            # перенесли при росте нагрузки на приложение
srv-4
srv-5
srv-6
```

Наступила ночь, город засыпает, трафик от пользователей снижается. Хотим посчитать фоновую статистику, которая произошла за день.  Приложению уже не требуется много ресурсов, ресурсы восвободились и мы можем посчитать всю статистику.

```
[hadoop_nodes]
srv-1
srv-2            # вернули при снижении нагрузки на приложение      
srv-3            # вернули при снижении нагрузки на приложение
srv-4            # перенесли для подсчета статистики
srv-5            # перенесли для подсчета статистики

[app_nodes]
srv-6
```

Такой способ самый простой, топорный, но имеет право на существование.

Следующая система, по наростанию сложости это Docker SWARM

#### Docker SWARM

<img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-19-10-18-11-image.png" alt="" data-align="center">

Логика следующая, есть некий мастер менеджер, который знает о всех наших машинах. В его памяти хранится файлик inventory (как в схемах в разделе выше) и он может обращаться к машинам и раздавать команды. Зная все машины, которые у него есть он ими оперирует и раздает команды где какой контейнер поднять.

Чтобы это показать работу Docker SWARM мы воспользуемся VirtualBox, в частности его оркестратором Vagrant.

##### VirtualBox + Vagrant

`cd 05-orchestration-swarm`

Vagrantfile представляет собой код на Ruby, здесь мы можем программировать конфигурацию из виртуальных машин, можем собрать оркестр. Описываем некий манифест на Ruby.

```ruby
Vagrant.configure("2") do |config|
  config.vm.box_check_update = false
  config.vbguest.installer_options = { allow_kernel_upgrade: true } # vagrant plugin install vagrant-vbguest

  #vibo: хотим поднять три машины
  (1..3).each do |i| #vibo: пишем цикл
    config.vm.define "node-#{i}" do |node|
      node.vm.box = "ubuntu/focal64" #vibo: первая машина master-swarm
      node.vm.hostname = "node-#{i}" #vibo: рабочий nod1
      node.vagrant.host = "node-#{i}" #vibo: рабочий nod2

      #vibo: программируем, чтобы у них были последовательные ip-адреса
      #vibo: пробросить порты последовательно, назначить имена
      node.vm.network "private_network", ip: "10.0.0.#{i+1}", virtualbox__intnet: "swarm"
      node.vm.network "forwarded_port", guest: 9090 + i, host: 9090 + i, host_ip: '127.0.0.1', protocol: 'tcp', auto_correct: true

      node.vm.provider "virtualbox" do |v|
        v.name = "node-#{i}"
      end

      #vibo: также Vagrant позволяет сразу накатывать нужные нам пакеты
      #vibo: c помощью технологии provision, существуют разные способы
      #vibo: самый простой это shell
      node.vm.provision "shell" do |s|
        s.inline = <<-SHELL
          #vibo: после того, как мы поднимем VirtualBox виртуалку, внутри
          #vibo: из них выполниться код, устанавливается докер
          set -e
          apt-get update
          apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common

          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
          add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

          apt-get update
          apt-get install -y docker-ce docker-ce-cli docker-compose containerd.io
          systemctl enable docker
          systemctl start docker
          usermod -a -G docker vagrant
        SHELL
        s.privileged = true
      end
    end
  end
end
```

> здесь были многочасовые танцы с бубнами после ошибки 404, при запуске vagrant up

`vagrant up` # запускаем машины

Процесс установки небыстрый, после повторного запуска:

```bash
Bringing machine 'node-1' up with 'virtualbox' provider...
Bringing machine 'node-2' up with 'virtualbox' provider...
Bringing machine 'node-3' up with 'virtualbox' provider...
==> node-1: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> node-1: flag to force provisioning. Provisioners marked to run always will still run.
==> node-2: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> node-2: flag to force provisioning. Provisioners marked to run always will still run.
==> node-3: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> node-3: flag to force provisioning. Provisioners marked to run always will still run.
```

Итого конфигурация следующая: на одной машине запущен master Docker SWARM, на двух других машинах запущены рабочие node. После запкуска машин спомощью метода provision на машинах устанавливается докер.

`lsl node-1` # заходим на машину 1 (в Терминале 1)

`vagrant ssh node-2` # заходим на машину 2 (в Терминале 2)

`vagrant ssh node-3` # заходим на машину 3 (в Терминале 3)

Чтобы создать кластер Docker SWARM его нужно инициализировать командой на мастер (node-1), 10.0.0.2 - интерфейс, который торчит наружу, чтобы наши ноды могли подключиться к мастеру (этот адрес прописан в установочном vagrant-файле):

`docker swarm init --advertise-addr 10.0.0.2` # инициализация Docker SWARM

```bash
Swarm initialized: current node (6eymyddx2k9vvnrahk174q2sh) is now a manager.

To add a worker to this swarm, run the following command:

    docker swarm join --token SWMTKN-1-37e06ru2uvpzstxdyy2z60jobvjtaeptpc6e0iag069nk5grpt-80q1itgfw8ecsp58kfecbo01e 10.0.0.2:2377

To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
```

После выполнения команды дается подсказка о том, чтобы присоединить машины нужно выполнить команду docker swarm join, указать индентификатор и указать хост, где запущен мастер, ip-адрес и порт:

`docker swarm join --token SWMTKN-1-37e06ru2uvpzstxdyy2z60jobvjtaeptpc6e0iag069nk5grpt-80q1itgfw8ecsp58kfecbo01e 10.0.0.2:2377` # присоединяем node-1, node-2 (в Терминале 2 и 3)

```bash
This node joined a swarm as a worker.
```

Ноды сообщают, что они присоединились к swarm в качестве рабочих нод. Выодим перечень нод:

`docker node ls` # вывод перечня node

```bash
ID                            HOSTNAME   STATUS    AVAILABILITY   MANAGER STATUS   ENGINE VERSION
6eymyddx2k9vvnrahk174q2sh *   node-1     Ready     Active         Leader           20.10.17
pb1jol2nftf7s4ig8rev1r54p     node-2     Ready     Active                          20.10.17
odhsw92copboelilp4r9miz2t     node-3     Ready     Active                          20.10.17
```

Видим, что у нас есть три рабочие node, одна из них лидер. Запустим сервис на нашем кластере:

`docker service create --replicas 3 --name shbr-echo cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest`

Команда похожа на docker-run, только вместо run здесь service create, также указывается сколько инстансев нам нужно запустить (--replicas 3). Указываем 3, чтобы на каждой ноде запкстилось по приложению.

```bash
kaic31q83rz6w1poifdn6hdkp
overall progress: 3 out of 3 tasks 
1/3: running   
2/3: running   
3/3: running   
verify: Service converged 
```

Выдим, что процесс завершен, переходим к node-2, node-3, мы на них ничего не делали, но приложение появилось:

`docker ps` #  в терминале 2 (node-2)

```bash
vagrant@node-2:~$ docker ps
CONTAINER ID   IMAGE                                                          COMMAND            CREATED         STATUS         PORTS       NAMES
aa995e231c64   cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest   "/entrypoint.sh"   3 minutes ago   Up 3 minutes   10000/tcp   shbr-echo.3.0f4yhk07lvkcs2dgnxfm6isit
```

`docker ps` # в терминале 3 (node-3)

```bash
vagrant@node-3:~$ docker ps
CONTAINER ID   IMAGE                                                          COMMAND            CREATED         STATUS         PORTS       NAMES
42e36ff0a2ca   cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest   "/entrypoint.sh"   4 minutes ago   Up 4 minutes   10000/tcp   shbr-echo.1.36p3m2b7j5k9s614peuykswfo
```

Список сервисов, которые мы поднимали можно вывести командой:

`docker service list` # список сервисов

```bash
vagrant@node-1:~$ docker service list
ID             NAME        MODE         REPLICAS   IMAGE                                                          PORTS
kaic31q83rz6   shbr-echo   replicated   3/3        cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest   
```

Здесь видим, что приложение shbr-echo запущено в 3/3 экземплярах.

Если хотим отмасштабировать наше приложение (исключить с одной машины):

`docker service scale shbr-echo=2` # масштабирование

```bash
vagrant@node-1:~$ docker service scale shbr-echo=2
shbr-echo scaled to 2
overall progress: 2 out of 2 tasks 
1/2: running   [==================================================>] 
2/2: running   [==================================================>] 
verify: Service converged
```

Командой `docker ps` проверяем и видим, что приложения нет на node-2. После этого нужно обновить сервис:

`docker service update --publish-add target=10000,published=10000 shbr-echo`

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-23-00-06-14-image.png" title="" alt="" data-align="center">

Теперь, сможем обратиться к нашим приложениям:

`curl http://localhost:10000/hello`

```
hello
```

При этом не смотря на то, что на node-3 приложения нет, ответ мы получаем.

```bash
vagrant@node-3:~$ docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES

vagrant@node-3:~$ curl http://localhost:10000/hello
hello
```

Последний пример с Docker SWARM, добавляем переменную окружения:

`docker service update --env-add PREFIX="World, " shbr-echo`

```bash
shbr-echo
overall progress: 2 out of 2 tasks 
1/2: running   
2/2: running   
verify: Service converged 
```

Контейнеры перезапустились. Переменная окружение доехала до нашего сервиса и в ответе она теперь присутствует:

`curl http://localhost:10000/hello`

```bash
World, hello
```

Останавливаем виртуальные машины командой:

`exit` # выходим из машины

`vagrant halt` # остановить виртуальные машины

```
==> node-3: Attempting graceful shutdown of VM...
==> node-2: Attempting graceful shutdown of VM...
==> node-1: Attempting graceful shutdown of VM...
```

Docker SWARM - давняя история, работает до определенного масштаба, простая архитектура, простой подход. На смену Docker SWARM пришел **kubernetes**.

#### kubernetes

Это более современная и продвинутая технология, пришла на смену Docker SWARM. Технология стала промышленным стандартом в оркестрации. Есть и другие инструменты, но сейчас рассмотрим его.

##### Структурная схема

![](/home/vibo/Pictures/GlobalMarkText/2022-08-23-00-07-27-image.png)

Есть master, в нем - api server к которому мы обращаемся. Соответственно, когда обращаемся к этому api server говорим, что нам нужен заданный контейнер в трех экземплярах. После этого эта информация записывается в базу данных - stcd (database). Далее мастер пытается сохранить систему в заданном состоянии. Т.е. если мы сказали, что нам нужно приложение в трех экземплярах, а какой-то из серверов выключился - то мастер сам поймет, что вместо трех экземпляров работают два и нужно срочно найти новую ноду, куда установить замену пропавшему серверу. Это обеспечивается с помощью компонет мастра - controllers и scheduler. Эти компоненты следят за конфигурацией и выполняют команды по определенному расписанию и пытаются распланировать, чтобы все команды выполнялись за приемлемое время. Есть разные конфигурации мастера, со 100% дублированием. Сейчас рассматриваем простую схему. Также есть node - это некие "рабочие лошадки", на которых расположены kubelet и kube-proxy. kubelet - принимает команды от api server и выполняет операции по поднятияю, опусканию контейнеров; kube-proxy - занимается маршрутизированием трафика на ноде.

##### Основные сущности

- **Pod** - минимальная единица развертывания, группа из одного или более контейнеров, собранных для совместного деплоя на ноде (т.е. один Pod мы не можем разместить на двух разных серверах, это что-то неделимое; в описании Pod есть список контейнеров, который нужно поднять и соответстветственно параметры этих контейнеров).

- **Deployment** - объект декларативрно-описывающий Podы, количество реплик и стратегию их замены при обновлении параметров (шаблон Poda и как его разворачивать, если какой-то Pod упадет - Deployment поднимент замену). 

- **Service** - инструмент для публикации приложения в качестве сетевого сервиса, в том числе реализующий балансировку нагрузки между Podами приложения (это декларация - публикуем концы наружу).

`cd 05-orchestration-k8s`

`vagrant up` # запускаем vagrant (или танцы с бубном или через vpn с ходу)

```bash
Bringing machine 'minikube' up with 'virtualbox' provider...
==> minikube: Machine already provisioned. Run `vagrant provision` or use the `--provision`
==> minikube: flag to force provisioning. Provisioners marked to run always will still run.
```

`vagrant ssh`

```bash
Welcome to Ubuntu 20.04.4 LTS (GNU/Linux 5.4.0-124-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Mon Aug 22 21:40:39 UTC 2022

  System load:  0.1               Processes:                133
  Usage of /:   6.3% of 38.70GB   Users logged in:          0
  Memory usage: 13%               IPv4 address for docker0: 172.17.0.1
  Swap usage:   0%                IPv4 address for enp0s3:  10.0.2.15


3 updates can be applied immediately.
2 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

New release '22.04.1 LTS' available.
Run 'do-release-upgrade' to upgrade to it.
```

`cd /vagrant/` # перемещаемся в папку из которой запускали vagrant

`ls`

```bash
Vagrantfile  hint.sh  k8s
```

##### minikube

Чтобы попробовать kubernetes локально можно воспользоваться командой minikube - это некая консольная утилита, которая позволяет на своем ноутбуке развернуть достаточно полноценный kubernetes

`minikube start --driver=none`

```bash
😄  minikube v1.23.2 on Ubuntu 20.04 (vbox/amd64)
✨  Using the none driver based on user configuration

🧯  The requested memory allocation of 2200MiB does not leave room for system overhead (total system memory: 2484MiB). You may face stability issues.
💡  Suggestion: Start minikube with less memory allocated: 'minikube start --memory=2200mb'

👍  Starting control plane node minikube in cluster minikube
🤹  Running on localhost (CPUs=2, Memory=2484MB, Disk=39630MB) ...
ℹ️  OS release is Ubuntu 20.04.4 LTS
🐳  Preparing Kubernetes v1.22.2 on Docker 20.10.17 ...
    ▪ kubelet.resolv-conf=/run/systemd/resolve/resolv.conf
🎉  minikube 1.26.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.26.1
💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

    > kubectl.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s
    > kubeadm.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s
    > kubelet.sha256: 64 B / 64 B [--------------------------] 100.00% ? p/s 0s
    > kubectl: 44.73 MiB / 44.73 MiB [---------------] 100.00% 2.57 MiB p/s 18s
    > kubeadm: 43.71 MiB / 43.71 MiB [---------------] 100.00% 2.33 MiB p/s 19s
    > kubelet: 146.25 MiB / 146.25 MiB [-------------] 100.00% 3.55 MiB p/s 41s
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
🤹  Configuring local host environment ...

❗  The 'none' driver is designed for experts who need to integrate with an existing VM
💡  Most users should use the newer 'docker' driver instead, which does not require root!
📘  For more information, see: https://minikube.sigs.k8s.io/docs/reference/drivers/none/

❗  kubectl and minikube configuration will be stored in /home/vagrant
❗  To use kubectl or minikube commands as your own user, you may need to relocate them. For example, to overwrite your own settings, run:

    ▪ sudo mv /home/vagrant/.kube /home/vagrant/.minikube $HOME
    ▪ sudo chown -R $USER $HOME/.kube $HOME/.minikube

💡  This can also be done automatically by setting the env var CHANGE_MINIKUBE_NONE_USER=true
🔎  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  Enabled addons: default-storageclass, storage-provisioner

❗  /snap/bin/kubectl is version 1.24.4, which may have incompatibilites with Kubernetes 1.22.2.
    ▪ Want kubectl v1.22.2? Try 'minikube kubectl -- get pods -A'
🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
```

Все нормально установилось, теперь можем обращаться к нашему kubernetes кластеру. Для управления кластером используется команда kubectl.

`kubectl get namespaces`

```bash
NAME              STATUS   AGE
default           Active   65s
kube-node-lease   Active   66s
kube-public       Active   66s
kube-system       Active   66s
```

`kubectl get namespaces -o yaml` # в yaml формате

```yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2022-08-22T21:49:47Z"
    labels:
      kubernetes.io/metadata.name: default
    name: default
    resourceVersion: "206"
    uid: 3c4f3cd6-3433-4993-87f1-c5d510b3167a
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2022-08-22T21:49:46Z"
    labels:
      kubernetes.io/metadata.name: kube-node-lease
    name: kube-node-lease
    resourceVersion: "43"
    uid: 7c360004-c01c-47ff-b2c4-3d1f32689287
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2022-08-22T21:49:46Z"
    labels:
      kubernetes.io/metadata.name: kube-public
    name: kube-public
    resourceVersion: "40"
    uid: 4557d02d-ca02-45cb-9cff-64c0a73afcd2
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
- apiVersion: v1
  kind: Namespace
  metadata:
    creationTimestamp: "2022-08-22T21:49:46Z"
    labels:
      kubernetes.io/metadata.name: kube-system
    name: kube-system
    resourceVersion: "15"
    uid: 6d4f5456-9460-46e4-b4d5-c63cff74f5ad
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active
kind: List
metadata:
  resourceVersion: ""
```

`kubectl get namespaces -o json` # в json формате

```json
{
    "apiVersion": "v1",
    "items": [
        {
            "apiVersion": "v1",
            "kind": "Namespace",
            "metadata": {
                "creationTimestamp": "2022-08-22T21:49:47Z",
                "labels": {
                    "kubernetes.io/metadata.name": "default"
                },
                "name": "default",
                "resourceVersion": "206",
                "uid": "3c4f3cd6-3433-4993-87f1-c5d510b3167a"
            },
            "spec": {
                "finalizers": [
                    "kubernetes"
                ]
            },
            "status": {
                "phase": "Active"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Namespace",
            "metadata": {
                "creationTimestamp": "2022-08-22T21:49:46Z",
                "labels": {
                    "kubernetes.io/metadata.name": "kube-node-lease"
                },
                "name": "kube-node-lease",
                "resourceVersion": "43",
                "uid": "7c360004-c01c-47ff-b2c4-3d1f32689287"
            },
            "spec": {
                "finalizers": [
                    "kubernetes"
                ]
            },
            "status": {
                "phase": "Active"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Namespace",
            "metadata": {
                "creationTimestamp": "2022-08-22T21:49:46Z",
                "labels": {
                    "kubernetes.io/metadata.name": "kube-public"
                },
                "name": "kube-public",
                "resourceVersion": "40",
                "uid": "4557d02d-ca02-45cb-9cff-64c0a73afcd2"
            },
            "spec": {
                "finalizers": [
                    "kubernetes"
                ]
            },
            "status": {
                "phase": "Active"
            }
        },
        {
            "apiVersion": "v1",
            "kind": "Namespace",
            "metadata": {
                "creationTimestamp": "2022-08-22T21:49:46Z",
                "labels": {
                    "kubernetes.io/metadata.name": "kube-system"
                },
                "name": "kube-system",
                "resourceVersion": "15",
                "uid": "6d4f5456-9460-46e4-b4d5-c63cff74f5ad"
            },
            "spec": {
                "finalizers": [
                    "kubernetes"
                ]
            },
            "status": {
                "phase": "Active"
            }
        }
    ],
    "kind": "List",
    "metadata": {
        "resourceVersion": ""
    }
}
```

##### namespace

Для демонстрации работы kubernetes создадим свой namespace, внутри namespace можно создавать любые объекты.

`kubectl create namespace shbr`

```
namespace/shbr created
```

Далее выполняем команду, которая внутри namespase shbr применяет файл deployment.yaml. В этом файле приведено описание объекта - Deployment, это та сущность, которая внутри содержит шаблон наших контейнеров, которые мы хотим запустить. Файл deployment.yaml:

```yaml
apiVersion: apps/v1 # имя api
kind: Deployment # имя сущности, объекта kubernetes
metadata: # метадата, которая может быть использована в программыных целях
  name: shbr-echo
  labels:
    app: shbr-echo
spec: # описание
  replicas: 1 # количество контейнеров с приложением, которое нужно запустить
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          imagePullPolicy: "Always"
          name: shbr-echo # название приложения
      restartPolicy: Always
```

##### deployment

`cd /vagrant/`

`ls`

```
Vagrantfile   hint.sh   k8s
```

Далее мы дословно просим внутри созданного namespaces - shbr применить файл deployment.yaml

`kubectl -n shbr apply -f k8s/deployment.yaml`

Kubectl отвечает, что такой deployment успешно сздан:

```
deployment.apps/shbr-echo created
```

Проверяем, список deployments:

`kubectl -n shbr get deployments`

Видим, что deployments shbr-echo - существует, 1/1 Pod готов, все доступно.

```bash
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
shbr-echo   1/1     1            1           96s
```

После создания delpoyments kubernetes создает pod. Посмотреть список Pod:

`kubectl -n shbr get pod`

```bash
NAME                        READY   STATUS    RESTARTS   AGE
shbr-echo-9cf75d6f6-vpmts   1/1     Running   0          5m23s
```

Може вывести информацию по этому Pod в yaml формате:

`kubectl -n shbr get pod shbr-echo-9cf75d6f6-vpmts -o yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-08-22T22:06:10Z"
  generateName: shbr-echo-9cf75d6f6-
  labels:
    app: shbr-echo
    pod-template-hash: 9cf75d6f6
  name: shbr-echo-9cf75d6f6-vpmts
  namespace: shbr
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet # owner это Poda
    name: shbr-echo-9cf75d6f6 # имя owner Poda
    uid: 77f77deb-da3c-4f8f-ade6-2ebab897b169
  resourceVersion: "1188"
  uid: 57c4443f-0df5-43c3-a591-567629bcfe57
spec:
  containers:
  - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
    imagePullPolicy: Always
    name: shbr-echo
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-8rvbc
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-8rvbc
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-08-22T22:06:10Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-08-22T22:06:53Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-08-22T22:06:53Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-08-22T22:06:10Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://f359cefaa759d4d00b1ae0f91ff4b86a5ac005573a79904cba5297edff60c158
    image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
    imageID: docker-pullable://cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker@sha256:ae5bb6451f39f4afb07fd7e0267864b97da092d85eac9ffbdf980fef3aa74022
    lastState: {}
    name: shbr-echo
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-08-22T22:06:52Z"
  hostIP: 10.0.2.15
  phase: Running
  podIP: 172.17.0.3
  podIPs:
  - ip: 172.17.0.3
  qosClass: BestEffort
  startTime: "2022-08-22T22:06:10Z"
```

Посмотрим на replicaset:

`kubectl -n shbr get replicaset shbr-echo-9cf75d6f6`

```
NAME                  DESIRED   CURRENT   READY   AGE
shbr-echo-9cf75d6f6   1         1         1       11m
```

`kubectl -n shbr get replicaset shbr-echo-9cf75d6f6 -o yaml`

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  annotations:
    deployment.kubernetes.io/desired-replicas: "1"
    deployment.kubernetes.io/max-replicas: "2"
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2022-08-22T22:06:10Z"
  generation: 1
  labels:
    app: shbr-echo
    pod-template-hash: 9cf75d6f6
  name: shbr-echo-9cf75d6f6
  namespace: shbr
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: Deployment # owner этого ReplicaSet - Deployment, который мы создали
    name: shbr-echo
    uid: b33fa9c2-45fc-4977-a9ac-f41afa8ac5c6
  resourceVersion: "1189"
  uid: 77f77deb-da3c-4f8f-ade6-2ebab897b169
spec:
  replicas: 1
  selector:
    matchLabels:
      app: shbr-echo
      pod-template-hash: 9cf75d6f6
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: shbr-echo
        pod-template-hash: 9cf75d6f6
    spec:
      containers:
      - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
        imagePullPolicy: Always
        name: shbr-echo
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  fullyLabeledReplicas: 1
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
```

**Мы создали объект Deployment, объект Deployment породил объект ReplicaSet, а объект ReplicaSet породил объект Pod**. И вот Pod - это минимальный объект (атом) в системе kubernetes и когда создается объект Pod - происходит аллокация контейнера на одной их машин в кластере. Этим занимаются контроллеры, они знают по apiVersion и по kind, что делать с тем или иным ресурсом. Наша задача внутри кластера создавать ресурсы, а контоллеры эти ресурсы уже обрабатывают.

Также есть тип ресурса service, который нужен для того, чтобы наш Pod, котрый мы подняли анонсировать наружу, чтобы к нему был доступ. Файл service.yaml:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: shbr-echo
  labels:
    app: shbr-echo
spec:
  type: NodePort
  ports:
  - port: 10000 # порт, который слушает контенер
    nodePort: 32000 # порт внутри сервиса
    protocol: TCP
  selector:
    app: shbr-echo
```

##### service

По аналогии с deployment создадим service

`kubectl -n shbr apply -f k8s/service.yaml`

```
service/shbr-echo created
```

`kubectl -n shbr get services`

```bash
NAME        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
shbr-echo   NodePort   10.109.158.125   <none>        10000:32000/TCP   51s
```

У нас создался сервис, внутренний ip у него 10.109.158.125. Порт 10000 - тот, который слушает контенер, а порт внутри сервиса это 32000, соответственно теперь внутри кластера мы можем вызвать по этому порту приложение:

`curl -v http://localhost:32000/hello`

И получаем ответ:

```bash
*   Trying 127.0.0.1:32000...
* TCP_NODELAY set
* Connected to localhost (127.0.0.1) port 32000 (#0)
> GET /hello HTTP/1.1
> Host: localhost:32000
> User-Agent: curl/7.68.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< X-Shbr-Lang: python
< X-Shbr-Lang-2: java
< Content-Type: text/plain; charset=utf-8
< Content-Length: 5
< Date: Mon, 22 Aug 2022 22:36:07 GMT
< Server: Python/3.8 aiohttp/3.8.1
< 
* Connection #0 to host localhost left intact
hello
```

##### Масштабирование

Увеличим количество наших Pod, чтобы посмотреть как это масштабируется. Обновим наш деплоймент и скажем, что нам нужно не одно приложени, а три, файл deployment.3.yaml:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shbr-echo
spec:
  replicas: 3 # три приложения
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          imagePullPolicy: "Always"
          name: shbr-echo
          env: # чтобы понять в какой контейнер попал запрос
            - name: PREFIX # в каждый контенер кладем переменную PREFIX
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP # значение переменной берем из системного словаря
      restartPolicy: Always
```

Выполняем команду:

`cd k8s`

`kubectl -n shbr apply -f deployment.3.yaml`

```
deployment.apps/shbr-echo configured
```

Проверяем, как отмаштабировались приложения:

`kubectl -n shbr get deployment`

```bash
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
shbr-echo   3/3     3            3           38m
```

Если посмотрим на список подов, то увидим три пода:

`kubectl -n shbr get pod`

```bash
NAME                         READY   STATUS    RESTARTS   AGE
shbr-echo-647887dd4f-4jjxw   1/1     Running   0          2m57s
shbr-echo-647887dd4f-dp55n   1/1     Running   0          2m25s
shbr-echo-647887dd4f-xlztg   1/1     Running   0          2m34s
```

Теперь curl будет возвращать ответ с ip-адресом, потому что мы его положили в переменную окуржения:

`curl -v http://localhost:32000/,hello`

```bash
*   Trying 127.0.0.1:32000...
* TCP_NODELAY set
* Connected to localhost (127.0.0.1) port 32000 (#0)
> GET /,hello HTTP/1.1
> Host: localhost:32000
> User-Agent: curl/7.68.0
> Accept: */*
> 
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< X-Shbr-Lang: python
< X-Shbr-Lang-2: java
< Content-Type: text/plain; charset=utf-8
< Content-Length: 16
< Date: Mon, 22 Aug 2022 22:48:17 GMT
< Server: Python/3.8 aiohttp/3.8.1
< 
* Connection #0 to host localhost left intact
172.17.0.6,hello
```

Если запустим в цикле нашу команду:

`watch -n 0.5 'curl -v http://localhost:32000/,hello'`

Видим, что каждые 500 мс меняется ip-адрес и наши приложения загружены равномерно.

Посмотрим, как ведет себя deployment, если лишить его одного из Pod:

`watch -n 0.5 'kubectl -n shbr get deployments'` # в терминале 1

`vagrant ssh` # в терминале 2

`cd /vagrant/`# в терминале 2

`kubectl -n shbr get pods`# в терминале 2

```bash
NAME                         READY   STATUS    RESTARTS   AGE
shbr-echo-647887dd4f-4jjxw   1/1     Running   0          15m
shbr-echo-647887dd4f-dp55n   1/1     Running   0          14m
shbr-echo-647887dd4f-xlztg   1/1     Running   0          14m
```

Удаляем третий Pod:

`kubectl -n shbr delete pod shbr-echo-647887dd4f-xlztg`

На какое-то время в Терминале 1 проскакивает AVAILABLE  2, вместо 3. Также можно посмотреть процесс при запущенной команде `watch -n 0.5 kubectl -n shbr get pod`.  Deployment это видит и поднимает еще один Pod, поддерживая исходную архитектуру. Можно с помощью команды *events* посмотреть логи, когда и какие Pod пришлось пересоздвать.

##### Важные поля

- **spec.containers[].resources.** Сколько ресурсов выдвавть контейнеру; если их не указать, то контейнер отъест все место на ноде и это будет очень плохо, потому, что кластер не понимает насколько ноды у него загружены и как более грамотно разместить контецнеры на ту или иную ноду.
  
  Также в kubernetes есть механизм автоскейлинга, когда он может в облаке докупать новые ноды. Мы можем ограничить железно ресурсы,  и дописать, сколько он может попросить из облака. Идея в том, что лимиты не дают утилизировать больше того или иного ресурса, а requests - это минимальное значение, которое требуется Pod.
  
  <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-24-20-24-46-image.png" title="" alt="" data-align="center">

- Еще есть **initContainers**, когда перед запуском своего контейнера можем совершить какие-то действия. Например, перед запуском приложения может скачать какой-нибудь файл.

- Также у контейнера есть понятия **livenessProbe** и **readnessProbe** - это некая команда или http запрос, которые отвечают на вопрос - приложение доступно сейчас или нет. Например, между приложением и базой данных что-то произошло и она не доступна для одной из Node. 
  
  В **readnessProbe** мы можем указать команду, которая тестирует доступна или нет база данных. И если эта команда говорит, что база данных недоступна, и readnessProbe упал, то тогда kubernetes этот контейнер выключит из балансоровки. Если **livenessProde** - более жесткая, если она упадет, то kubernetes удалит и перезапустит Pod целиком.
  
  <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-24-20-25-57-image.png" title="" alt="" data-align="center">

##### Стратегии деплоя

Кофигурация производится в соответствующем yaml-файле. Затем он применяется командой `kubectl -n shbr apply -f ИМЯ_deployment.yaml`. Spec файла нужно как-то менять для повторного запуска, чтобы kubernetes выполнил редеплой.

###### Recreate

Рестартим все приложения сразу. Файл deployment.3.recreate.yaml:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shbr-echo
spec:
  replicas: 3
  strategy:    # добавили поле
    type: Recreate # и здесь добавили Recreate
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          imagePullPolicy: "Always"
          name: shbr-echo
          env:
            - name: PREFIX
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: RESTART # здесь
              value: "1" # и здесь
      restartPolicy: Always
```

Делаем так, что spec контейнеров как-то изменилась и kubernetes понял, что нужно произвести редеплой нашего приложения по страетегии Recreate.

У нас упали сраз все три контейнера. Происходит быстрая перезагрузка все трех контейнеров разово, но в этот момент трафик проливается. Когда все контейнеры пропали никто не может обрабатывать трафик пользователей.

###### Rolling

Более щедящий подход - убивается за раз только треть контейнеров, файл deployment.3.rolling.yaml:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shbr-echo
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 33%    # максимально можно убить треть контецнеров!
      maxUnavailable: 33%
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          imagePullPolicy: "Always"
          name: shbr-echo
          env:
            - name: PREFIX
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: RESTART
              value: "2"
      restartPolicy: Always
```

В данной схеме kubernetes будет перезапускать по одному контейнеру. Такой режим **всегда** сохраняет один контейнер рабочий. Такая схема требует больше ресурсов. Все зависит от области применения. Recreate - можно использовать для фоновых задач, эта история не для пользователей.

###### BlueGreen

Самая дорогая схема, когда мы целиком поднимаем новую версию приложения и переключаем трафик на нее. Просто она не реализуется. Без танцев с бубном не обойтись.

У нас есть две spec, отличаются только названием и дополнительной меткой.

Файл deployment-1.yaml:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shbr-echo-1 # название
spec:
  replicas: 3
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
        version: "1" # метка
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          name: shbr-echo
          env:
            - name: PREFIX
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
      restartPolicy: Always
```

Файл deployment-2.yaml:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: shbr-echo-2 # название
spec:
  replicas: 3
  selector:
    matchLabels:
      app: shbr-echo
  template:
    metadata:
      labels:
        app: shbr-echo
        version: "2" # метка
    spec:
      containers:
        - image: cr.yandex/crpamim8dasm0u97qtpf/shbr-01-package-docker:latest
          name: shbr-echo
          env:
            - name: PREFIX
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
      restartPolicy: Always
```

Сервис предназначен для того, чтобы делать публиацию наших Pod. Мы можем развернуть деплоймент-2 и внутри сервиса переключить селектор по которому kubernetes понимает куда нужно лить трафик. Файл service.yaml:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: shbr-echo-bluegreen
  labels:
    app: shbr-echo-bluegreen
spec:
  type: NodePort
  ports:
  - port: 10000
    nodePort: 32001 # порт
    protocol: TCP
  selector:
    app: shbr-echo
    version: "1"    # меняем руками на 2, когда нужно переключить 
```

Создаются две тройки контейнеров. Теперь переходим в service.yaml и меняем selector version, весь трафик теперь льется через deployment-2. Теперь вручную удаяем deployment-1. Самая дорогая история.

##### Инструменты

Для работы в kubernetes есть два основных инструмента  HELM и  Kustomizer, Kustomizer входит сейчас в kubectl в стандартную поставку. С помощью этих инструментов можно менять файлы, положить в репозиторий чарты и т.д.

Для управления конфигурацией:

###### HELM

В HELM все спеки, которые мы писали руками можно шаблонизировать. Т.е. у нас есть спека и файл с переменными и мы можем эти переменные редактировать, менять и накатывать. Захотели поменять переменную окружения - нам не надо идти глубоко внуть всех спек, искать, где эта перменная задается. Поменяли заранее заданную переменнтую и это могло привести к изменению множества спек. Здесь можно писать include, if. 

###### Kustomizer

Kustomizer - более затейливый. В нем есть базовая спека, в зависимости от окружения, мы можем на этую скепу накладываются патчи. Эти патчи будут накладываться на базовую и рекурсивно меняться в глубину.

Для системного администрирования:

###### k9s

`k9s -A`

<img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-23-14-45-13-image.png" alt="" data-align="center">

Как kubectl, только работает в интерактивном режиме. И все те ресурсы которые мы запрашиваем в kubectl можем запрашивать здесь. Суперфича - мы можем интерактивно проваливаться внутрь контейнера (hot-key s, т.е shell). Также доступен просмотр логов, stdout контейнера можно также удобно смотреть.

**Как попробовать в проде**

В Яндекс-облаке есть meneged-kubernates-servise. Можно создать кластер, подключиться к нему снаружи. После создания кластера есть создание рабочей группы и kubernetes созадет node автоматически (во вкладке compute-cloude).

### Вопросы

Как на уровне ОС ограничиваются ресурсы доступные приложению. Контейнерная виртуализация, в частности докер - это по сути процесс внутри процесса и линукс с помощью механизма си-груп может управлять сколько и какому контейнеру выделять времени.

Чтобы описать некоторый сервис yaml файл создается вручную. Далее через kubectl apply мы передаем их в api. Поверх kubertetes есть уже много приложений, в которых можно накликать вручную.

Как достучаться до Pada снаружи кластера. За это отвечает сервис, параметр nodePort: 32001 # порт. Есть три типа сервиса:

- NodePort (эквивалентен Docker SWARM);

- ClusterIP (это только внутренний ip в кластере);

- LoadBalanser (самый сложный и дорогой, внешний ip). 

### 7. CI/CD

Автоматизируем все это дело.

**Time to Market** - это время, которое компания тратит на реализацию и выпуск бизнес-идеи своим клиентам. Чем ниже time to market, тем быстрее фича отправляется в продакшн и приноси прибыль.

С помощью CI/CD сервисов мы выгружаем приложение в репозитоий и оно выезжает в прод.

Мы будем рассматривать CI - gitlab.[ссылка](https://docs.gitlab.com/ee/ci/quick_start/). CI в гитлабе представляет собой список операций, похож на Ansible. Пример ниже предусматривает четыре операции, каждая из которых разнесена на stage.

##### .gitlab-ci (пример)

Описание пайплайна. Файл `.gitlab-ci.yml` - пример:

```yaml
build-job:
  stage: build
  script:
    - echo "Hello, $GITLAB_USER_LOGIN!"

test-job1:
  stage: test
  script:
    - echo "This job tests something"

test-job2:
  stage: test
  script:
    - echo "This job tests something, but takes more time than test-job1."
    - echo "After the echo commands complete, it runs the sleep command for 20 seconds"
    - echo "which simulates a test that runs 20 seconds longer than test-job1"
    - sleep 20

deploy-prod:
  stage: deploy
  script:
    - echo "This job deploys something from the $CI_COMMIT_BRANCH branch."
```

##### Vagrantfile

Пробовать GitLab будем локально, описание Vagrantfile:

```bash
Vagrant.configure("2") do |config|
    config.vm.box_check_update = false
    config.vbguest.installer_options = { allow_kernel_upgrade: true } # vagrant plugin install vagrant-vbguest
    # заводим порты внутрь виртуальной машины
    config.vm.network "forwarded_port", guest: 8080, host: 8080, host_ip: '127.0.0.1', protocol: 'tcp', auto_correct: true
    config.vm.network "forwarded_port", guest: 2222, host: 22022, host_ip: '127.0.0.1', protocol: 'tcp', auto_correct: true

    config.vm.define "ci" do |node| # название виртуальной машины
        node.vm.box = "ubuntu/focal64"
        node.vm.hostname = "ci"
        node.vagrant.host = "ci"

        node.vm.provider "virtualbox" do |v|
            v.name = "ci"
            v.memory = 6144 # выделяем машине побольше памяти
            v.cpus = 5
        end

        node.vm.provision "shell" do |s|
        s.inline = <<-SHELL
        set -e
        apt-get update
        apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common

        curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
        add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

        # устанавливаем докер
        apt-get update
        apt-get install -y docker-ce docker-ce-cli docker-compose containerd.io conntrack
        systemctl enable docker
        systemctl start docker
        usermod -a -G docker vagrant

        # устанавливаем minikube
        curl -L https://github.com/kubernetes/minikube/releases/download/v1.23.2/minikube_1.23.2-0_amd64.deb > minikube_1.23.2-0_amd64.deb
        dpkg -i minikube_1.23.2-0_amd64.deb
        sudo -u vagrant minikube start --driver=none

        snap install kubectl --classic
        curl -sS https://webinstall.dev/k9s | sudo -u vagrant bash

        # устанавливаем GitLab ранер
        curl -L "https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh" | sudo bash
        apt-get update
        apt-get install -y gitlab-runner
        usermod -a -G docker gitlab-runner

        mv ~/.bash_logout ~/.bash_logout.bak 2>/dev/null || true
        SHELL
        s.privileged = true
        end
    end
end
```

`vagrant up`

`vagrant ssh`

`cd /vagrant/`

`ls`

```bash
Vagrantfile         gitlab-runner.service  k8s
docker-compose.yml  hint.sh                virtualbox
```

##### docker-compose

GitLab поднимем с помощью докер-контейнера, файл docker-compose.yml:

```yml
version: "2.4"

services:
  gitlab: # объявляем некий сервис gitlab
    image: gitlab/gitlab-ce:14.3.3-ce.0 # сервис поднимать из этого контейнера
    restart: always # если упадет - перезапускать
    hostname: 'localhost'
    environment: # задаем переменные окружения, для конфигурации gitlab
      GITLAB_ROOT_EMAIL: "shbr@yandex-team.ru" # логин
      GITLAB_ROOT_PASSWORD: "yandex-shbr" # пароль
      GITLAB_OMNIBUS_CONFIG: | # модифицируем конфиг, т.к. есть конфликт по портам
        external_url 'http://localhost:8080'
        gitlab_rails['gitlab_shell_ssh_port'] = 2222
        puma['port'] = 8081
    container_name: gitlab
    ports:
      - '8080:8080'
      - '2222:22'
    volumes: # lсоздаем дирректирии, чтобы при перезапуске эти файлы не терялись
      - '/gitlab/etc:/etc/gitlab'
      - '/gitlab/logs:/var/log/gitlab'
      - '/gitlab/data:/var/opt/gitlab'
  demo:
    image: ubuntu:focal
    container_name: demo
    command:
      - /bin/tail
      - -f
      - /dev/null
```

Поднимается docker-compose командой `docker-compose up` (если один файл) или полностью с указанием имени файла:

`docker-compose -f docker-compose.yml up -d`

```
Starting demo ... 
Starting demo ... done
```

Флаг -d, чтобы докер запустился, а консоль вернулась нам.

`docker ps`

Видим, что запустилось много контейнеров, в т.ч. c GitLab. Смотрим на логи.

`docker logs -f 5962e680b906` # контейнер GitLab

Видим очень большую конструкцию. Как работает gitlab c точки зрения конфигурирования. В начале есть строка *Chef Infra Client* - эта система управления конфигурацией, система старая, аналог Ansible. В системе множество различных сервисов: бэкенд api, фронтенд со всеми кнопочками, postges для данных, git shell и т.д. Разработчики gitlab написали множество Cookbooks (их перечисление есть в логах). Видим, как рендерятся наши конфиги, нам остается только задавать переменные окружения в файл docker-compose.yml. Когда мы задаем их в файле Chief их прочитал и в конфиги отрендерил, положил на диск. Когда gitlab запустится эти переменные будут внутри конфигов. Это очень удобно. Дальше можно увидеть конфиги postgrs. В логах пошли http-запросы. GitLab поднялся.

##### Браузер

`http://localhost:8080/users/sign_in`

Вводим логин (shbr@yandex-team.ru) и пароль (yandex-shbr), создаем новый проект.

![](/home/vibo/Pictures/GlobalMarkText/2022-08-24-22-41-53-image.png)

##### push в gitlab

Теперь нам нужно что-то положить в GitLab, чтобы запустить CI. Хотим перенести весь наш репозиторий с примерами в GitLab. В папке shbr-devops-master выполняем команды по инструкции gitlab:

`git init --initial-branch=main`

`git remote add origin ssh://git@localhost:22022/root/shbr.git` # здесь важно указать порт 22022, в инструкции изначально был порт 2222

`git add .`

`git commit -m "Initial commit"`

Проверяем файл shbr-devops-master/.git/config

```bash
[core]
    repositoryformatversion = 0
    filemode = true
    bare = false
    logallrefupdates = true
[remote "origin"]
    url = ssh://git@localhost:22022/root/shbr.git # порт 22022!!!
    fetch = +refs/heads/*:refs/remotes/origin/*
```

Отправляем файлы в gitlab (не забываем, что все это делается локально). 

Соответственно перед push нужно еще добавить свой публичный ssh-ключ в настройки локального gitlab. В процессе соглашаемся и пишем yes.

`git push -u origin main`

```bash
The authenticity of host '[localhost]:22022 ([127.0.0.1]:22022)' can't be established.
ED25519 key fingerprint is SHA256:jO+lut6L+8ATEwkbUprphQexMzubapj1xvFtosWgAOM.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '[localhost]:22022' (ED25519) to the list of known hosts.
Enumerating objects: 121, done.
Counting objects: 100% (121/121), done.
Delta compression using up to 8 threads
Compressing objects: 100% (95/95), done.
Writing objects: 100% (121/121), 73.48 KiB | 9.18 MiB/s, done.
Total 121 (delta 20), reused 0 (delta 0), pack-reused 0
remote: Resolving deltas: 100% (20/20), done.
To ssh://localhost:22022/root/shbr.git
 * [new branch]      main -> main
branch 'main' set up to track 'origin/main'.
```

Итого файлы в репозиторий gitlab загружены, в корне репозитория лежит файл .gitlab-ci.yml.

![](/home/vibo/Pictures/GlobalMarkText/2022-08-24-23-24-28-image.png)

Видим, что статус Pipeline: Pending (в ожидании). Переходим на вкладку pipelines, видим метку stuck (застрял), переходим во вкладку jobs там выбираем pending и смотрим на причину:

```
This job is stuck because you don't have any active runners that can run this job.
Go to project CI settings

Это задание зависло, потому что у вас нет активных ранеров, которые могли бы запустить это задание.
Перейти к настройкам CI проекта
```

##### specific runners

Переходим по ссылке в сообщении на вкладку CI/CD. Нам предлагают установить gitlab runner и указать хост и некий секрет.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-24-23-33-29-image.png" title="" alt="" data-align="center">

Проверим, что наше приложение работает. Переходим в папку 06-gitlab откуда запускали vagrant

`vagrant ssh` # подключаемся к машине

`cd /vagrant/`

Отправляем запрос приложению

`curl http://localhost:8080/`

Приложение отвечает

```bash
<html><body>You are being <a href="http://localhost:8080/users/sign_in">redirected</a>.</body></html>
```

GitLab runner мы прописывали ранее в файле Vagrantfile. После выполнения в контейнере создается репозиторий gitlab, идем туда.

vagrant@ci: `/etc/apt$ cd /etc/apt/sources.list.d/`

Видим, что здесь лежит runner

```
runner_gitlab-runner.list
```

`cat runner_gitlab-runner.list`со

```bash
# this file was generated by packages.gitlab.com for
# the repository at https://packages.gitlab.com/runner/gitlab-runner

deb [signed-by=/usr/share/keyrings/runner_gitlab-runner-archive-keyring.gpg] https://packages.gitlab.com/runner/gitlab-runner/ubuntu/ focal main
deb-src [signed-by=/usr/share/keyrings/runner_gitlab-runner-archive-keyring.gpg] https://packages.gitlab.com/runner/gitlab-runner/ubuntu/ focal main
```

Видим deb-пакеты подписанные, и src-пакеты (содержат исходный код и часто распространяются вместе с кодом, чтобы можно было скачать пакет с кодом, что-то подправить и собрать оригинальный пакет, особенно это удобно в rpm).

##### gitlab-runner

Т.к. мы уже заранее поставили runner нам нужно только его зарегистрировать

`gitlab-runner register`

Получаем интерактивный опросник. Копируем url, который нам выдали (http://localhost:8080/), секретный токен (oNeTcSVffMkcrR1wCWVv), описание, теги если используется несколько раннеров это удобно. И главный вопрос про executor - где мы будем выполнять команды, пишем shell. Тут много вариантов.

Итого нам говорят, что конфигурация сохранена в /home/vagrant/.gitlab-runner/config.toml

```bash
Runtime platform                                    arch=amd64 os=linux pid=20672 revision=bbcb5aba version=15.3.0
WARNING: Running in user-mode.                     
WARNING: The user-mode requires you to manually start builds processing: 
WARNING: $ gitlab-runner run                       
WARNING: Use sudo for system-mode:                 
WARNING: $ sudo gitlab-runner...                   

Enter the GitLab instance URL (for example, https://gitlab.com/):
http://localhost:8080/
Enter the registration token:
oNeTcSVffMkcrR1wCWVv
Enter a description for the runner:
[ci]: ci
Enter tags for the runner (comma-separated):

Enter optional maintenance note for the runner:

Registering runner... succeeded                     runner=oNeTcSVf
Enter an executor: docker, docker-ssh, parallels, virtualbox, docker-ssh+machine, kubernetes, custom, shell, ssh, docker+machine:
shell
Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded!

Configuration (with the authentication token) was saved in "/home/vagrant/.gitlab-runner/config.toml" 
```

Можно этот файл было и руками создать, как удобнее.

`cat /home/vagrant/.gitlab-runner/config.toml`

```bash
concurrent = 1 # раннер может осущетсвлять только одну сборку параллельно
check_interval = 0

[session_server]
  session_timeout = 1800

[[runners]]
  name = "ci"
  url = "http://localhost:8080/"
  id = 1
  token = "8L95ksppRjAHhDzdoxJt"
  token_obtained_at = 2022-08-24T20:57:55Z
  token_expires_at = 0001-01-01T00:00:00Z
  executor = "shell"
  [runners.custom_build_dir]
  [runners.cache]
    [runners.cache.s3]
    [runners.cache.gcs]
    [runners.cache.azure]
```

Обновляем страницу [Sign in · GitLab](http://localhost:8080/root/shbr/-/settings/ci_cd#js-runners-settings)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-25-00-04-02-image.png" title="" alt="" data-align="center">

Раннер добавился. Он имеет статус specific, что значт он прибит к конкретному проекту. Заходим внутрь, видим разную информацию о раннере. Главную, Last contact - Never, это значит, что раннер еще обращался к gitlab.

##### Запуск

Теперь задача - запустить.

`mkdir  /home/vagrant/gitlab-runner`

`/usr/bin/gitlab-runner run --working-directory /home/vagrant/gitlab-runner`

```
Runtime platform                                    arch=amd64 os=linux pid=22855 revision=bbcb5aba version=15.3.0
Starting multi-runner from /home/vagrant/.gitlab-runner/config.toml...  builds=0
WARNING: Running in user-mode.                     
WARNING: Use sudo for system-mode:                 
WARNING: $ sudo gitlab-runner...                   

Configuration loaded                                builds=0
listen_address not defined, metrics & debug endpoints disabled  builds=0
[session_server].listen_address not defined, session endpoints disabled  builds=0
Checking for jobs... received                       job=1 repo_url=http://localhost:8080/root/shbr.git runner=8L95kspp
```

Пошел процесс установки. Статус на странице gitlab изменился.

Проблема - консоль занята gitlab runner. Вспоминаем, как запускать сервисы. Т.к. мы внутри vagrant виртуалки - спомощью systemd запустим наш runner.

Выходим `ctrl`+`C`

`cd /vagrant/`

`cat gitlab-runner.service`

```bash
[Unit]
Description=gitlab-runner

[Service]
ExecStart=/usr/bin/gitlab-runner run --working-directory /home/vagrant/gitlab-runner

[Install]
WantedBy=default.target
```

У нас есть сервис-файл и наша задача положить его в нужное место. Каждый пользователь может создавать под себя сервисы. Наш будет именно таким.

`cat hint.sh` # смотрим файл с подсказкой

```bash
#!/bin/bash

mkdir -p ~/.config/systemd/user
cp gitlab-runner.service ~/.config/systemd/user/gitlab-runner.service

systemctl --user daemon-reload
systemctl --user start gitlab-runner
```

Положим файл в домашнюю дирректорию ~/.config/systemd/user.

`mkdir -p ~/.config/systemd/user`

`cp gitlab-runner.service ~/.config/systemd/user/gitlab-runner.service`

Запускаем сервис:

`systemctl --user start gitlab-runner`

Проверяем, что сервис запустился:

`systemctl --user status gitlab-runner`

```bash
● gitlab-runner.service - gitlab-runner
     Loaded: loaded (/home/vagrant/.config/systemd/user/gitlab-runner.service; disabled; vendor preset:>
     Active: active (running) since Wed 2022-08-24 21:25:22 UTC; 10s ago
   Main PID: 41426 (gitlab-runner)
     CGroup: /user.slice/user-1000.slice/user@1000.service/gitlab-runner.service
             └─41426 /usr/bin/gitlab-runner run --working-directory /home/vagrant/gitlab-runner

Aug 24 21:25:22 ci systemd[2192]: Started gitlab-runner.
Aug 24 21:25:22 ci gitlab-runner[41426]: Runtime platform                                    arch=amd64>
Aug 24 21:25:22 ci gitlab-runner[41426]: Starting multi-runner from /home/vagrant/.gitlab-runner/config>
Aug 24 21:25:22 ci gitlab-runner[41426]: WARNING: Running in user-mode.                    
Aug 24 21:25:22 ci gitlab-runner[41426]: WARNING: Use sudo for system-mode:                
Aug 24 21:25:22 ci gitlab-runner[41426]: WARNING: $ sudo gitlab-runner...                  
Aug 24 21:25:22 ci gitlab-runner[41426]:                                                   
Aug 24 21:25:22 ci gitlab-runner[41426]: Configuration loaded                                builds=0
Aug 24 21:25:22 ci gitlab-runner[41426]: listen_address not defined, metrics & debug endpoints disabled>
Aug 24 21:25:22 ci gitlab-runner[41426]: [session_server].listen_address not defined, session endpoints>
```

Видим, что Started gitlab-runner запущен. Выходим `ctrl`+`C`. Можем посмотреть логи:

`sudo journalctl -u gitlab-runner`

##### gitlab-ci

Посмотрим, что мы хотим добиться от нашего CI (файл .gitlab-ci.yml):

```yaml
build:
  stage: build
  script:
    - cd presentation/01-package-docker
    - make docker-build

cd:
  stage: deploy
  script:
    - cd presentation/01-package-docker
    - echo $DOCKER_PASSWORD | docker login --username json_key --password-stdin cr.yandex
    - make docker-push

ci:
  stage: deploy
  needs:
    - cd
  script:
    - kubectl -n shbr rollout restart 'deployment/shbr-echo'первый этап сборки (build); для этого мы переходим в паку 
```

- первый этап сборки (build), для сборки нам нужно перейти в дирректорию presentation/01-package-docker и запустить docker-build;

- второй этап; дальше каждый step стартует из корня репозитория, соответственно пред тем как что-то делать на нужно заново переходить в дирректорию, идем в presentation/01-package-docker, логинимся в docker-registry и отправляем туда наш образ.

- третий этап - запускаем рестарт нашего приложения.

##### DOCKER_PASSWORD

Для того, чтобы пушить внутрь репозитория нужно залогиниться. Создадим переменную DOCKER_PASSWORD. Сам пароль нужно заранее сгенерировать и положить в корень репозитория key.json

##### ИТОГО

У нас есть три шага:

- сборка (build)
  
  На этапе сборки мы собираем наш контейнер. Редактируем файлы в репозитории и они попадают в докер-контейнер, который мы собираем.

- доставка (deploy - cd)
  
  После сборке на шаге cd контейнер отправляется в репозиторий, перед тем как отправить его в репозиторий нам нужно залогиниться. Это обязательно.

- деплой (deploy - ci)
  
  После того, как мы запушили обновление мы сообщаем kubernetes, чтобы он перезапустил наш деплоймент. Kubernetes за счет imagePullPolicy: "Always" - перескачает наш образ и перезапустит приложение.

Таким образом код, который мы только что запушили выезжает на продакшн.
