## Асинхронное программирование. Часть 1.

Первоисточник: [Часть 1](https://www.youtube.com/watch?v=rMh5O4eZrug), [Часть 2](https://www.youtube.com/watch?v=rYQk3PW16bE)

**В чем разберемся:**

- Что не так с синхронными приложениями

- Как это исправить асинхронностью

- Что не так с асинхронными приложениями

Код будет приведен на псевдопитоне.

### Что не так с синхронными приложениями

**Что такое веб-приложение:**

- Получить запрос от клиента

- Бизнс-логика

- Отправить ответ клиенту

**Пишем приложение:**

- Покажет какие заведения в данном городе

- Найдет ближайшие места по предпочтениям

- Оповестит об акциях и особых предложениях

#### Структура

В качестве основы берем написание приложения на Flask. Примерная структура.

```python
# импоритируем flask
from flask import Flask, request
# инициализируем наше приложение
app = Flask(__name__)

# здесь какая-то функция бизнесс-логики
# которая обрабатывает входящий json
def buisiness_logic():
    pass

@app.rout('/')
def hello():
    # здесь обрабатываем входящий json (request.json, запрос клиента)
    # с помощью написанной выше функции бизнес-логики
    response = buisiness_logic(request.json)
    # возвращаем конечный результат (отпарвляем клиенту)
    return response

if __name__ = '__main__':
    app.run()
```

#### Бизнесс-логика

Что из себя будет представлять функция бизнесс-логики

```python
# на вход приходит запрос в виде json - req_json 
def buisiness_logic(req_json):
    date = req_json['date']
    # будем получать события через некий request.get
    event = request.get(f'https://eda.yandex.ru/{date}').json()
    # для получения информации по place идем в базу данных
    place = db.places.find_by_categories(date)
    # аналогично получим menu_items через базу данных
    menu_items = db.items.find_by_place_id(date)

    # на выходе хотим получить какие-то события, места, каталог
    return json.dumps({
        'event': event,
        'place': place,
        'menu_items': menu_items,
    })
    # итого получаем json.dumps
```

**Где здесь взаимодействие с клиентом?**

- *Получить запрос от клиента* - request.json

- Бизнс-логика

- *Отправить ответ клиенту* - response

#### Что такое socket?

**Socket - абстракция ОС, позволяющая свести любое сетевое взаимодействие к вызовам write и read.**

Т.е. основная задача - передаем на выполнение ту или иную задачу, операционная система сама ее выплняет и потом возвращает результат.

#### Низкоуровневая реализация

```python
# заимпортим os и socket
import os, socket

# основная функция
def server():
    # создадим серверный сокет в адресном пространстве IPv4
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # adress space IPv4
    # установим порт, адрес
    sock.bind(('127.0.0.1', 8888) # specifying location
    # включим режим пассивного ожидания тех или иных запросов через sock.listen
    sock.listen() # switching to passive mode
    while True:
        conn, addr = sock.accept()
        request_data - conn.read(1024)
        response = buisiness_logic (request_data)
        conn.sendall(b"""\r\nHTTP/1.1 200 OK\r\n\r\n""" + response)
        conn.close()

server()
```

Заменим эти три строчки:

```python
    # создадим серверный сокет в адресном пространстве IPv4
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) # adress space IPv4
    # установим порт, адрес
    sock.bind(('127.0.0.1', 8888) # specifying location
    # включим режим пассивного ожидания тех или иных запросов через sock.listen
    sock.listen() # switching to passive mode
```

на один метод:

```python
sock = create_passive_socket()
```

Итого получим:

```python
# заимпортим os и socket
import os, socket

# основная функция
def server():
    sock = create_passive_socket()
    while True:
        # у нас есть conn и addr и через sock.accept мы принимаем первое содинение из очереди
        conn, addr = sock.accept() # accept first connection from queue
        # читаем полученнные данные методом conn.read на 1024 байта
        request_data = conn.read(1024)
        # получаем результат с помощью buisiness_logic
        response = buisiness_logic(request_data)
        # отправляем пользователю ответ через conn.sendall
        conn.sendall(b"""\r\nHTTP/1.1 200 OK\r\n\r\n""" + response)
        # закрываем соединение
        conn.close() # finish handling this request
        # ПОСЛЕ ЭТОГО МЫ МОЖЕМ ПРИНИМАТЬ СЛЕУДЮЩЕЕ СОДИНЕНИЕ
```

#### Что такое системный вызов (syscall)

- Процесс передает управление ОС

- ОС делает что-то полезное

- Процесс ожидает пока ОС вернет урпавлени обратно

#### Явный syscall

`request_data = conn.read(1024)` # <mark>system call</mark>

Здесь очевидно, когда данные читаются.

#### Syscall не виден, а он есть

В функции buisiness_logic():

`request.get` # когда получаем запросы <mark>system call</mark>

`db.places` # обращение к базе данных <mark>system call</mark>

`db.items` # обращение к базе данных <mark>system call</mark>

На более глубоком уровне эти методы так или иначе подразумевают вызов system call.

### Что если два запроса придут одновременно?

ОБработка запросов будет последовательная. Мы через sock.accept() прочитаем первый запрос, затем сделаем conn.read, отработаем buisiness_logic, отправим response и закроем соединение. Только после этого перейдем к следующему запросу.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-31-22-52-27-image.png" title="" alt="" data-align="center">

Почему так происходит, из-за чего мы блокируемся? Дело в def server() - conn.read, sendall и response. Потому, что пока не будет прочитан весь запрос (1024 байта) мы будем ждать, пока наш системный вызов не отработает.

Т.е. если у нашего клиента проблемы со связью - мы можем эти 1024 байта ждать очень долго. Все остальные запросы будут его ождать. 

Для def buisiness_logic() проблемы теже в request.get и при обращениях к базам данных. На этих этапах мы будем блокироваться.

##### Решение: процессы

- Будем создавать по процессу на каждое соединение

- ОС будет их переключать

- PROFIT!

```python
# так же функция сервер
def server():
    # создаем серверный сокет
    sock = create_passive_socket()
    # через цикл делаем accept()
    while True:
        conn, addr = sock.accept()
        # через multiprocessing тригерим handler
        multiprocessing.Process(
            target=handler, args=[conn]
        ).start()
        conn.close()

# хэндлер выполняет бизнесс-логику и возвращает результат
# НЕ БЛОКИРУЯСЬ В ОЖИДАНИИ
def handler(conn):
    request_data = conn.recv(1024)
    response = buisiness_logic(request_data)
    conn.sendall(b"""\r\nHTTP/1.1 200 OK\r\n\r\n""" + response)
    conn.close() 
```

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-31-23-08-56-image.png" title="" alt="" data-align="center">

Теперь процессы обрабатываются параллельною

##### Проблемы процессов в высоконагруженных веб-приложениях

- Требуют много памяти (особенно для Python)

- Неэффективная коммникация (чтобы обеспечить взаимодействие двух процессов нам скорее всего понадобится тертий процесс)

- Оверхед со стороны ОС

Есть ли что-то дешевле процессов?

##### Решение: потоки

- Требуют меньше памяти, чем процессы

- Более эффективная коммуникация

- Более легковесное переключение

Потоки - по сути концепция точно такая же как с прцессами, но потоки требуют меньше памяти. Т.к. у потоков общая память, то и взаимодействие их более эффективное.

- Будем создавать по ~~процессу~~ потоку на каждое соединение

- ОС будет их переключать

- PROFIT!

Как это будет выглядеть:

```python
# так же функция сервер
def server():
    # создаем passive_socket
    sock = create_passive_socket()
    # через цикл делаем accept() по сокету
    while True:
        conn, addr = sock.accept()
        # создаем по треду на отдельный хендлер (отдельное соединение)
        # которое будет выполняться и работать одновременно
        threading.Thread(
            target=handler, args=[conn]
        ).run()

# хэндлер выполняет бизнесс-логику и возвращает результат
# НЕ БЛОКИРУЯСЬ В ОЖИДАНИИ
def handler(conn):
    request_data = conn.recv(1024)
    response = buisiness_logic(request_data)
    conn.sendall(b"""\r\nHTTP/1.1 200 OK\r\n\r\n""" + response)
    conn.close()
```

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-08-31-23-20-32-image.png" title="" alt="" data-align="center">

Треды выполняются параллельно.

##### Проблемы потоков в высоконагруженных веб-приложениях на Python

- Другие проблемы с коммуникацией (гонки, дедлоки...); гонки могут происходить из-за того, что мы будем не аккуратно работать с потоками и т.к. мы имеем общую память возникает гонка за ресурс; этоможет создавать неопределенное поведение и всевозможным способом нас аффектить. Дедлоки еще хуже - мы можем просто заблокироваться и остановиться.

- Все еще оверхед со стороны ОС (шедулинг)

- Даже если ограничить их количество пулами - пулы забиваются, приложение встает

- В Python еще и GIL 

##### Global Interpreter Lock (GIL)

Global Interpreter Lock (GIL) - не дает двум потокам интерпретатора выполняться параллельно. Это нужно в первую очередь для того, чтобы защитить наши данные, нашу память от различных гонок и перезаписывания.

Например, как это работает. <u>Эти два потока никогда не будут выполняться одновременно</u>:

```python
def super_mult(n):
    res = 0
    for i in range(n ** 10):
        res += res * i
    return res
```

```python
def super_sum(n):
    res = 0
    while res < 100:
        res += n
    return res
```

```python
t1 = Thread(target=super_mult, args=[100).run()
t2 = Thread(target=super_sum, args=[100).run()
```

По принципу GIL они не будут выполняться одновременно.

**Зачем нам GIL?**

- Структуры данных интерпретатора не являются потокобезопасными

- Данные нужно защищать от гонок

<u>Эти потоки будут выполняться параллельно</u> через вызов ThreadPool():

```python
def foo(x):
    return requests.get(f'http://example-site.com/{x}')

ThreadPool().map(foo, range(100 000))
```

По соответсвующей функции мы каждый вызов (каждый return) будем делать параллельно.

И эти:

```python
import numpy as np

def foo(x):
    np/eye(x) * np/eye(x)

ThreadPool().map(foo, range(100 000))
```

И здесь, пример с numpy, когда под капотом реализация на языке С - в нем также в ThreadPool() выполнять будут параллельно.

**Есть ли жизнь после GIL?**

- наличие GIL не означает, что потоками в Python нельзя пользоваться

- Более того, некоторые вещи можно сделать только при помощи потоков (работа с файловой системой, сторонние либы)

**И все же...**

- GIL вносит дополнительный оверхед

- Про GIL надо знать

- По возможности проводите нагрузочное тестирование и делайте бенчмарки

**Что имеем на данный момент:**

- Хотим обрабатывать несколько запросов одновременно

- Можно при помощи процессов

- Можно при помощи потоков

- У них есть недостатки, которые иногда мешают

##### Чем занимаются потоки

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-01-09-47-58-image.png" title="" alt="" data-align="center">

<u>Почему запросы ждут предыдущих - есть блокирующие системные вызовы.</u>

**Мы блокируемся на input/output (I/O)**

- ввод/вывод

- так называют взаимодействие с "внешним" миром

```python
conn_data = conn.read(1024) # network
file_data = fileobj.read(1024) # filesystem
```

Получается, что мы очень много ждем. Нас это не устраивает. Нагрузка на Python минимальная относительно времени ввода/вывода.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-01-09-51-30-image.png" title="" alt="" data-align="center">

##### Идеальный мир

Идеальный мир: избавиться от всех простоев и постоянно заниматься только чем то полезным, экономя наши ресурсы и время. И идеальном мире в Thread1 происходит отбработка тех или иных запросов.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-01-09-53-37-image.png" title="" alt="" data-align="center">

Есть разные способы решения проблемы:

- закидываем по каждому отдельно треды на обработку и у нас огромный простой

- или в асинхронной модели (async) мы спомощью дополнительных инструментов обрабатываем и неперывно держим нашу загрузку

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-01-09-57-27-image.png" title="" alt="" data-align="center">

Пробуем реализовать идеальный мир. Для упрощения сделаем while с условием data_is_not_ready_for_read - если мы не готовы читать запустим другой процесс - handle_other_requests. После этого делаем conn.read, потому, что мы не хотим блокироваться. Дальше отрабатываем нашу бизнес-логику.

```python
import os, socket

def server():
    sock = create_passive_socket()
    while True:
        conn, addr = sock.accept() # 
        while data_is_not_ready_for_read(conn): # 1
            handle_other_requests() # 2 
        request_data = conn.read(1024) # we don't want to block
        response = buisiness_logic(request_data)
        conn.sendall(b"""\r\nHTTP/1.1 200 OK\r\n\r\n""" + response)
        conn.close()
```

##### Неблокирующий вывод-вывод

ОС может сообщить о том, что I/O заблокирует поток. Это происходит через setblocking(). Т.е. если рассматриваем нормальную операционную систему, вывов fcntl с соответсвующими флагами. В такой модели мы можем учитывать отработали мы с ошибкой, что нам для этого нужно, надо ли прочитать еще, надо ли отправить или перейти заниматься другими запросами.

```python
import os, socket

def server():
    sock = create_passive_socket()
    while True:
        conn, addr = sock.accept()
        conn.setblocking(0) # fcntl (fd, F_SETFL, flags | 0_NONBLOCR)
        while True:
            pass
            try:
                conn.sendall(response)
            except socket.error as se: #se.errno == EAGAIN (*)
        pass
```

Не очень круто - нам нужно тратить по системному вызову на каждый сокет. Это крайне большие накладные расходы.

##### Поллинг

В него передаются три типа дескрипторов.

- Системный вызов select

- В него передается три множества дескрипторов
  
  - Те, из которых я хочу **прочитать (read_fds)**
  
  - Те, из которых я хочу **написать (write_fds)**
  
  - Те, из которых возможно что-то **сломалось (exc_fds)**
  
  - ОС, скажи, где я могу реализовывать свое желание и не заблокироваться? С **таймаутом (timeout)**.

```python
rlist, wlist, elist = select(read_fds, write_fds, exc_fds, timeout)
```

Попробуем реализовать поллинг в нашем решении.

```python
def server():
    sock = create_passive_socket()
    server.setblocking(0)
    # принимаем соединение как только оно прийдет
    rlist = [server] # we can accept only if there is a conn
    wlist, elist = [], []
    # запускаем поллинг
    while True:
        rs, ws, es = select.select(rlist, wlist, elist, 0.1) # polling
        # обрабатываем соединение
        for rsock in rs:
            # если новое соединение
            if rsock is server: # new connection has appeared
                # читаем событие
                new_read_sock, _ = rsock.accept() # reading event
                # переключаемся в неблокирующий режим
                new_read_sock.serblocking(0) # switching to non-blocking mode
                # говорим, что мы хотим прочитать
                rlist.append(new_read_sock) # we want to read
            # если соединение не новое
            else:
                # отрабатывает какой-то сокет
                handle_read(rsock) # magic
        for wsock in ws:
            handle_write(wsock)
        pass 
```

**Что мы пытаемся сдлеать - мы хотим взять на себя ответственность за переключение задач.** Мы в ручном режиме переключаемся и тем самым хотим невилировать наши простои.

Воспользуемся реализацией funcs_to_call для сохранения наших call-back для каждого сокета. И будем говорить, что если rsock это server, то будем записывать в мапу конкретный handle_read. Т.е на конкретный файл дескриптора сокета мы говорим, что нужно вызвать конкретный хэндлер на обработку. Тем самым мы его запоминаем и делаем после этого append на новое чтение.

```python
def server():
    ...
    funcs_to_call = {} # let's store callbacks for each socket!
    while True:
        rs, ws, es = select.select(rlist, wlist, elist, 0.1) 
        for rsock in rs:            
            if rsock is server:               
                new_read_sock, _ = rsock.accept()
                funcs_to_call[new_read_sock] = handle_read # to remember handler                
                rlist.append(new_read_sock)           
            else:
                funcs_to_call[new_read_sock](
                    new_read_sock, rs, ws, es, funcs_to_call
                )
        ...
```

Рассмотрим функцию handle_read из def server(). Мы гарантируем, что операционная система не заблокируется. Т.е. она послушно выполнит read на 1024 байта, после этого отрабатывает бизнес-логика. Далаем получение ивента через ресурс. Делаем ег оappend и после этого обрабатываем наш ответ.

```python
def handle_read(socket, rs, ws, es, funcs_to_call):
    request = socket.read(1024) # guaranteed by OS not to block
    data = json.loads(request)['data']
    event_socket = request.get(f'https://eda.yandex.ru/{date}')
    rs.append(event_socket)
    funcs_to_call[event_socket] = handle_got_event # processing response
    # processed event, generated a new event

def handke)got_event(socket, rs, ws, es, funcs_to_call)
```

Важный момент! Мы создали новое событие, его обработали и после этого затригерили еще одно новое событие на его обработку. По новому событию вошли в обработчик, вошли в funcs_to_call и вызвали новый обработчик handle_got_event. Тем самым породили новое событие, в котором мы както обрабатываем и прогнозируем некоторые действия.

##### Мы изобрели Event Loop

- имеется поток событий (или сообщений)

- в ответ на событие нужно запустить обработчик

- обработчики в процессе выполнения могут подождать новые события.

Примеры событий: из сокета можно прочитать без блокировки, был принят сигнал, сработал таймер, завершилась какая-то функция.

Понятно, что в программе таких обработчиков может быть сколько угодно...

```python
# создаем even_loop
loop = create_even_loop()
# создаем сервер по адресу и порту
server = loop.create_server(('127.0.0.1', 8888))

loop.add_connected_callback(server, callback=handle_connected)

# вызываем первый основной обработчик
def handle_connected(loop, conn): # when the connection appears
    loop.add_read_callback(conn, callback=handle_read)

def handle_read(loop, data): # we can red
    response = business_logic(loop, data)
    loop.add_write_callback(conn, response, callback=handle_write_ok)

def handle_write_ok(loop, conn, nbytes):
    logger.info(f'for {conn} succesfully send {nbytes} bytes')
```

Обрабатываем события без ожидания последовательно непрерывно выполняем запросы.

##### Это база

- доверять нельзя никому и ничему
  
  - любой сервис может пятисотнуть, отдать что-то нехорошее или затаймаутить

- не строить ложных иллюзий и ложных предположений
  
  - "это поле наверняка обязательное"
  
  - "да тут не может быть null"

- код должен быть такой, как будто железо и окружающие тебя разработчики тебя ненавидят :)

- в атмосфере ненависти и недоверия рождаются стабильные и отказоустойчивые системы 

##### Что будет если мы где-то сломались?

Например, отрабатываем def handle_connected() и что-то произошло в read_callback(). Тогда нам нужны дополнителные обработчики - succes_callback, error_callback, handle_read_error и так по-хорошему в каждом месте. 

```python
def handle_connected(loop, conn):
    loop.add_read_callback(conn, 
        on_succes_callback=handle_read,
        on_error_callback=handle_read_error)

def handle_read(loop, data):
    response = business_logic(loop, data)
    loop.add_write_callback(conn, response, callback=handle_write_ok)

def handle_read_error(loop, conn, error):
    logger.info(f'Stopped {conn}: {error}')
    loop.close_connection(conn)

def handle_write_ok(loop, conn, nbytes):
    logger.info(f'for {conn} succesfully send {nbytes} bytes')
```

Мы должны все залогировать, если что-то пошло не так, чтобы предпринять нужные действиия. Получаем огромное количество разных ситуаций, в которых нужно обработать ошибки. Опять же разные ошибки - на нашей стороне или на стороне сервера - обрабатываются по разному. Так мы начинаем заростать хендлерами это называется **CALLBACK HELL**.

Чтение кода намного усложняется, структура теряется, легко допустить ошибку.

Синхронная веррсия выглядела так красиво и просто: У нас есть бизнес-логика, у нас есть date, который мы формируем из json. Мы делаем запрос на ресурс eda.yandex.ru и делаем два запроса по place и menu_items к нашей базе данных. Возвращаем ответ пользователю json.dumps.

```python
def buisiness_logic(req_json):
    date = req_json['date'] 
    event = request.get(f'https://eda.yandex.ru/{date}').json()
    place = db.places.find_by_categories(date)
    menu_items = db.items.find_by_place_id(date)

    return json.dumps({
        'event': event,
        'place': place,
        'menu_items': menu_items,
    })
```

Нет сложной структуры. С бизнес-логикой аналогично. А в асинхронной версии мы запутались.

##### Корутины, или async и await спешат на помощь

Грубо говоря, мы добавляем async и оставляем синхронный код.

```python
async def buisiness_logic(req_json):
    date = req_json['date'] 
    event = request.get(f'https://eda.yandex.ru/{date}').json()
    place = db.places.find_by_categories(date)
    menu_items = db.items.find_by_place_id(date)

    return json.dumps({
        'event': event,
        'place': place,
        'menu_items': menu_items,
    })
```

**Но так было не всегда...**

```python
@gen.coroutine
def buisiness_logic(req_json):
    date = req_json['date'] 
    event = yield client.get(f'https://eda.yandex.ru/{date}').json()
    place = yield db.places.find_by_categories(date)
    menu_items = yield db.items.find_by_place_id(date)

    raise gen.Return(json.dumps({
        'event': event,
        'place': place,
        'menu_items': menu_items,
    }))
```

Как перейти от коллбеков к async/await посмотрим на следующей лекции.

##### За счет чего асинхронные приложения могут быть более производительными?

Приложение переносит на себя ответственность за переключение задач. Чтобы избежать простоя, мы можем переключать задачи самостоятельно, устранив простои.

### Проблемы асинхронного программирования

- синхронный и асинхронный код не живут вместе
  
  - <u>проблема: </u>для некоторых задач отсутствуют асинхронные библиотеки
  
  - <u>решение: </u>1) Threadpool, 2) Написать самим

- не для всего есть асинхронные интерфейсы

- CPU-bound задачи
  
  - <u>проблема:</u> event-loop должен постоянно вращаться, если кто-то заблокирует - встанет все приложение
  
  - <u>решение: </u>1) process pool, 2) отдельный микросервис

- иногда RPS ограничен не веб-сервером
  
  - <u>проблема: </u>если ваше приложение в 99% случаев ходит в базу данных, то ваша производительность равна производительности базы данных (сервер держит 99999 RPS, база держит 10 QPS)
  
  - <u>решения:</u> необходимо продумывать архитектуру

Есть веб-сервис, который выполняет какую-то логику. В микросервисной архитектуре у нас могут быть сотни или даже тысячи всевозможных запросов к различным ресурсам, базам данных. Если мы в модели асинхронного программирования переложим CPU-bound задачи (кагда выполняем что-то тяжело и долго), то возникает проблема в нашей модели, потому, что мы начинаем блокироваться не потому, что мы ждем, а потому, что мы что-то делаем, вычисляем. Поэтому в этой модели важно делать акцент на архитектуре. На то, что асинхронные веб-сервисы всевозможным образом делают то или иное взаимодействие, а CPU-bound за дачи выплняются строго на отдельных микросервисах.

### Итого

##### Когда использовать асинхронщину

- микросервисы (I/O-bound, не CPU-bound задачи)

- долгоживущие соединения (websocket, раздача файлов), когда мы не хотим закрывать соединение, реализация мессенджера, чата

- есть производительная архитектура (кеши, write-heavy очереди)

- модель экономит ресурс серверов

##### Когда не использовтаь асинхронщтну

- CPU-bound

- боттлнек в инфраструктуре (один инстанс базы)

- если вы можете себе позволить сколько угодно железа...
